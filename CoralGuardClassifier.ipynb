{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNpnpF8pu8Wn"
      },
      "source": [
        "# 필요한 라이브러리 install 및 import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7Tglh3LjA9j"
      },
      "outputs": [],
      "source": [
        "!pip install kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxTdBZfjgTHf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2buVxI2vEm8"
      },
      "source": [
        "# 압축 해제 및 데이터셋 분할"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W1rFXGZtruZ"
      },
      "outputs": [],
      "source": [
        "zip_path = \"/content/coral_dataset.zip\"\n",
        "extract_folder = \"coral_dataset\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
        "    zipf.extractall(extract_folder)\n",
        "\n",
        "bleached_folder = os.path.join(extract_folder, \"bleached\")\n",
        "normal_folder = os.path.join(extract_folder, \"normal\")\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "def load_images_from_folder(folder, label):\n",
        "    for filename in os.listdir(folder):\n",
        "        file_path = os.path.join(folder, filename)\n",
        "        try:\n",
        "            img = Image.open(file_path).convert(\"RGB\")\n",
        "            img = img.resize((64, 64))\n",
        "            images.append(img)\n",
        "            labels.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "\n",
        "load_images_from_folder(bleached_folder, label=1)\n",
        "load_images_from_folder(normal_folder, label=0)\n",
        "\n",
        "train_images, temp_images, train_labels, temp_labels = train_test_split(\n",
        "    images, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "val_images, test_images, val_labels, test_labels = train_test_split(\n",
        "    temp_images, temp_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_images = np.array(train_images)\n",
        "train_labels = np.array(train_labels)\n",
        "val_images = np.array(val_images)\n",
        "val_labels = np.array(val_labels)\n",
        "test_images = np.array(test_images)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "train_images = np.array(train_images)  # 리스트를 numpy 배열로 변환\n",
        "print(train_images.shape)  # 결과 확인\n",
        "\n",
        "print(\"Train Images:\", len(train_images))\n",
        "print(\"Validation Images:\", len(val_images))\n",
        "print(\"Test Images:\", len(test_images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqCodzRP8PCk"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "val_test_datagen = ImageDataGenerator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMkjOVB8wNY4"
      },
      "source": [
        "# 본격 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW3iiTIJwVDW"
      },
      "source": [
        "직접 제작하고 추가한 함수/메커니즘:\n",
        "1. Activation Function (i): **OutOut**\n",
        "2. Activation Function (ii): **YooSeong Function** `(in final classification)`\n",
        "3. **IOU Loss Function**\n",
        "4. **Self-Attention Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7Fy2MHR2r4a"
      },
      "outputs": [],
      "source": [
        "def outout(inputs, num_units, dropout_rate=0.3, training=True):\n",
        "\n",
        "    if training:\n",
        "        inputs = tf.nn.dropout(inputs, rate=dropout_rate)\n",
        "\n",
        "    return tf.reduce_max(tf.reshape(inputs, [-1, num_units, 2]), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLuGbXHs2_7v"
      },
      "outputs": [],
      "source": [
        "class YooSeongActivation(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(YooSeongActivation, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return (tf.tanh(inputs) ** 2) * tf.exp(inputs) / (tf.exp(inputs) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hhMQoDgwfwo"
      },
      "outputs": [],
      "source": [
        "def YooSeong(x):\n",
        "  return (tf.tanh(x) ** 2) * tf.exp(x) / (tf.exp(x) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz2d44PX2uVt"
      },
      "outputs": [],
      "source": [
        "def IOU_Loss(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, y_pred.dtype)\n",
        "    intersection = tf.reduce_sum(tf.minimum(y_true, y_pred), axis=-1)\n",
        "    union = tf.reduce_sum(tf.maximum(y_true, y_pred), axis=-1)\n",
        "    iou = intersection / (union + 1e-7)\n",
        "    loss = 1 - iou\n",
        "    return tf.reduce_mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTMwYsmY-SXv"
      },
      "outputs": [],
      "source": [
        "# Self-Attention Layer의 구현은 생성형 AI ChaTGPT의 도움을 받음.\n",
        "class SelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.softmax = layers.Softmax(axis=-1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Query, Key, Value 계산\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "\n",
        "        # Attention Scores 계산\n",
        "        attention_scores = tf.matmul(query, key, transpose_b=True)\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "\n",
        "        # Attention Output 계산\n",
        "        attention_output = tf.matmul(attention_weights, value)\n",
        "        return attention_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM9WmoWs-fEC"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "\n",
        "    layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1), activation='relu', input_shape=(64, 64, 3)),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    SelfAttention(embed_dim=32),\n",
        "\n",
        "    layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    SelfAttention(embed_dim=64),\n",
        "\n",
        "    layers.Flatten(),\n",
        "\n",
        "    layers.Dense(6 * 24 * 24 * 2),\n",
        "    layers.Lambda(lambda x: outout(x, 6 * 24 * 24, dropout_rate=0.2)),\n",
        "\n",
        "    layers.Dense(2),\n",
        "    YooSeongActivation()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnfaxkIR_cps"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS4XdUvzDPi0"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQEqQFQFDSDZ"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint(filepath='best_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.00001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZNRHX84DUZw"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_datagen.flow(train_images, train_labels, batch_size=32),\n",
        "    validation_data=(val_images, val_labels),\n",
        "    epochs=50,\n",
        "    callbacks=[early_stopping, model_checkpoint, learning_rate_reduction]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBaAYlEuDfBu"
      },
      "source": [
        "# Test Dataset 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ-dHm1XDrKT",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(test_images)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9ssLLKhD2iz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for i, idx in enumerate(np.random.choice(range(len(test_images)), 5)):\n",
        "    plt.subplot(4, 2, i + 1)\n",
        "    plt.imshow(test_images[idx], cmap='gray')\n",
        "    plt.title(f\"True: {test_labels[idx]}, Pred: {y_pred_classes[idx]}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS9D-assD5Id"
      },
      "source": [
        "# 최종 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNuAgz9BEAdJ"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmK_uPdbEF5m"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 1, figsize=(10, 5))\n",
        "ax[0].plot(history.history['loss'], label=\"Training Loss\")\n",
        "ax[0].plot(history.history['val_loss'], label=\"Validation Loss\")\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(history.history['accuracy'], label=\"Training Accuracy\")\n",
        "ax[1].plot(history.history['val_accuracy'], label=\"Validation Accuracy\")\n",
        "ax[1].legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coral_Located"
      ],
      "metadata": {
        "id": "kAHBkS3_PmR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_images = []\n",
        "test_folder = \"coral_located\"\n",
        "for filename in os.listdir(test_folder):\n",
        "        file_path = os.path.join(test_folder, filename)\n",
        "        try:\n",
        "            img = Image.open(file_path).convert(\"RGB\")\n",
        "            img = img.resize((64, 64))\n",
        "            test_images.append(img)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "test_images = np.array(test_images)\n",
        "print(test_images.shape)\n",
        "\n",
        "prediction = model.predict(test_images)\n",
        "print(prediction)\n",
        "'''for filename in os.listdir(test_folder):\n",
        "    file_path = os.path.join(test_folder, filename)\n",
        "    img_array = Image.open(file_path).convert(\"RGB\")\n",
        "    if img_array is not None:\n",
        "        # 배치 차원 추가\n",
        "        print(img_array)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        print(img_array.shape)\n",
        "        # 모델 예측\n",
        "        prediction = model.predict(img_array)\n",
        "        # 백화 확률 (클래스 1에 대한 확률)\n",
        "        bleached_probability = prediction[0][1]\n",
        "        # 결과 저장\n",
        "        results.append({\"filename\": filename, \"bleached_probability\": bleached_probability})'''\n",
        "\n",
        "# 결과를 DataFrame으로 변환\n",
        "results_df = pd.DataFrame(prediction)\n",
        "\n",
        "# 결과를 CSV 파일로 저장\n",
        "results_csv_path = \"predictions.csv\"\n",
        "results_df.to_csv(results_csv_path, index=False)\n",
        "\n",
        "print(f\"Predictions saved to {results_csv_path}\")"
      ],
      "metadata": {
        "id": "yfru1RsVPqD8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}